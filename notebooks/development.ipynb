{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00567662-db55-438d-957f-a6c89015a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5eedadb-4631-4955-973b-14c11336a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# F√ºge /app zum Suchpfad hinzu\n",
    "sys.path.append(os.path.abspath(\"../app\"))\n",
    "\n",
    "from input_manager import load_resume_data, extract_clean_text_from_pdf, find_csv_and_pdf_files\n",
    "from rag_manager import init_chroma_db, get_collection, add_dataframe_to_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ee0756-449d-4906-913b-d90fac3d7a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Gefundene CSV-Dateien: 1\n",
      "üìÑ Gefundene PDF-Dateien: 1\n",
      "‚úÖ Es wurden 34 Berufserfahrungen erkannt. Der Zeitraum erstreckt sich von January 2010 bis June 2025 (5655 Tage).\n",
      "‚úÖ Qualit√§tspr√ºfung bestanden.\n",
      "‚úÖ Die Stellenauschreibung wurde erkannt.\n"
     ]
    }
   ],
   "source": [
    "csv_path, pdf_path = find_csv_and_pdf_files('../data/input')\n",
    "df = load_resume_data(csv_path[0])\n",
    "pdf_text = extract_clean_text_from_pdf(pdf_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55eb9b5d-eab9-46b1-be1c-7b8ad6dae625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Chroma-Verzeichnis gefunden. Vorhandene Daten werden geladen.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The CHROMA_OPENAI_API_KEY environment variable is not set.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Init DB\u001b[39;00m\n\u001b[32m      2\u001b[39m db = init_chroma_db()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m collection = \u001b[43mget_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbewerbung\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# CSV laden\u001b[39;00m\n\u001b[32m      6\u001b[39m df = load_resume_data(\u001b[33m\"\u001b[39m\u001b[33mdata/input/resume_data.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/app/rag_manager.py:22\u001b[39m, in \u001b[36mget_collection\u001b[39m\u001b[34m(client, name)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_collection\u001b[39m(client, name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mexperience_db\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Gibt die gew√ºnschte Collection zur√ºck, legt sie ggf. an.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     embedding_function = \u001b[43mOpenAIEmbeddingFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m client.get_or_create_collection(name=name, embedding_function=embedding_function)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/chromadb/utils/embedding_functions/openai_embedding_function.py:63\u001b[39m, in \u001b[36mOpenAIEmbeddingFunction.__init__\u001b[39m\u001b[34m(self, api_key, model_name, organization_id, api_base, api_type, api_version, deployment_id, default_headers, dimensions, api_key_env_var)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key \u001b[38;5;129;01mor\u001b[39;00m os.getenv(api_key_env_var)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api_key:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_key_env_var\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m environment variable is not set.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m.model_name = model_name\n\u001b[32m     66\u001b[39m \u001b[38;5;28mself\u001b[39m.organization_id = organization_id\n",
      "\u001b[31mValueError\u001b[39m: The CHROMA_OPENAI_API_KEY environment variable is not set."
     ]
    }
   ],
   "source": [
    "# Init DB\n",
    "db = init_chroma_db()\n",
    "collection = get_collection(db, name=\"bewerbung\")\n",
    "\n",
    "# CSV laden\n",
    "df = load_resume_data(\"data/input/resume_data.csv\")\n",
    "\n",
    "# In RAG speichern\n",
    "add_dataframe_to_chroma(df, collection, source_id=\"resume_2025_06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8014c7e1-e217-44c5-a320-52260675394a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "865edeca-5d39-4f81-a83c-c5dcdc128617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kurzprofil erstellen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0f45568-66f6-4a62-8f84-a2223a11c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9758b278-99f8-433a-9e28-f7c772d596ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(prompt_type: str, language: str = \"de\") -> str:\n",
    "    prompts = {\n",
    "        \"reduce_pdf\": {\n",
    "            \"de\": \"\"\"Du erh√§ltst den extrahierten Rohtext einer Stellenanzeige (PDF). Bitte filtere **nur** die relevanten Informationen heraus:\n",
    "\n",
    "- Jobtitel\n",
    "- Aufgabenbereiche\n",
    "- Anforderungen (F√§higkeiten, Erfahrung, Ausbildung)\n",
    "- Erw√ºnschte Qualifikationen\n",
    "\n",
    "Ignoriere irrelevante Inhalte wie:\n",
    "- Navigationsmen√ºs der Website\n",
    "- Kontaktinformationen\n",
    "- Rechtliche Hinweise\n",
    "- Hinweise zum Bewerbungsprozess\n",
    "\n",
    "Gib die Informationen als strukturierte Markdown-Liste mit klaren √úberschriften zur√ºck (z.‚ÄØB. **Jobtitel**, **Aufgaben**, ...).\n",
    "\n",
    "Input:\n",
    "\\\"\\\"\\\"\n",
    "{pdf_text}\n",
    "\\\"\\\"\\\"\"\"\",\n",
    "\n",
    "            \"en\": \"\"\"You will receive the raw extracted text of a job posting (PDF). Please filter out **only** the relevant information:\n",
    "\n",
    "- Job title\n",
    "- Responsibilities\n",
    "- Requirements (skills, experience, education)\n",
    "- Preferred qualifications\n",
    "\n",
    "Ignore irrelevant content such as:\n",
    "- Website navigation elements\n",
    "- Contact information\n",
    "- Legal notices\n",
    "- Application instructions\n",
    "\n",
    "Return the information as a structured markdown list with clear headings (e.g., **Job Title**, **Responsibilities**, ...).\n",
    "\n",
    "Input:\n",
    "\\\"\\\"\\\"\n",
    "{pdf_text}\n",
    "\\\"\\\"\\\"\"\"\"\n",
    "        },\n",
    "        \"select_experiences\": {\n",
    "            \"de\": \"\"\"Du bist Expert:in f√ºr Karriereberatung. Basierend auf der folgenden Stellenanzeige und einer Liste bisheriger Berufserfahrungen sollst du die **drei relevantesten Erfahrungen** ausw√§hlen, die am besten zu den Anforderungen passen.\n",
    "\n",
    "### Stellenanzeige:\n",
    "\\\"\\\"\\\"\n",
    "{bereinigter_jobtext}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "### Berufserfahrungen (Tabellenformat):\n",
    "\\\"\\\"\\\"\n",
    "{csv_als_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Bitte gib f√ºr jede der drei gew√§hlten Erfahrungen einen kurzen Abschnitt zur√ºck mit:\n",
    "\n",
    "- Jobtitel\n",
    "- Aufgaben/Rolle\n",
    "- Zeitraum (falls vorhanden)\n",
    "- Warum diese Erfahrung besonders relevant f√ºr die Stelle ist\n",
    "\n",
    "Formuliere die Antwort so, dass sie direkt in einer Bewerbung verwendet werden kann.\"\"\",\n",
    "\n",
    "            \"en\": \"\"\"You are an expert in career consulting. Based on the following job description and a list of past work experiences, please identify the **three most relevant experiences** that best match the job requirements.\n",
    "\n",
    "### Job Description:\n",
    "\\\"\\\"\\\"\n",
    "{bereinigter_jobtext}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "### Work Experiences (Table Format):\n",
    "\\\"\\\"\\\"\n",
    "{csv_als_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Please return a short paragraph for each of the three selected experiences including:\n",
    "\n",
    "- Job title\n",
    "- Tasks/Role\n",
    "- Duration (if available)\n",
    "- Why this experience is especially relevant to the job\n",
    "\n",
    "Write the response so it can be directly used in an application.\"\"\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        return prompts[prompt_type][language]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"‚ùå Prompt oder Sprache nicht gefunden: '{prompt_type}' in '{language}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8a672ca-5c83-4ff9-b78d-3418277bd6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prompt_within_token_limit(prompt: str, model: str = \"gpt-4\", max_tokens: int = 8192) -> bool:\n",
    "    \"\"\"\n",
    "    Pr√ºft, ob der gegebene Prompt unterhalb des Tokenlimits liegt.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Der gesamte Prompt (z.‚ÄØB. inkl. PDF-Text, CSV-Text usw.)\n",
    "        model (str): Modellname (\"gpt-4\", \"gpt-3.5-turbo\", etc.)\n",
    "        max_tokens (int): Maximale erlaubte Anzahl an Tokens (inkl. Antwort)\n",
    "\n",
    "    Returns:\n",
    "        bool: True, wenn innerhalb des Limits, sonst False\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(prompt))\n",
    "    print(f\"üìè Prompt enth√§lt {num_tokens} Tokens (Limit: {max_tokens})\")\n",
    "    return num_tokens < max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b326eb-90ba-4fcc-af64-b86a1eb360aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str, model: str = \"gpt-4\") -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    token_count = len(tokens)\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07deaf24-0ac8-456f-9002-3e645a2e959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_prompt_length(prompt: str, model: str = \"gpt-4\", max_tokens: int = 8192) -> None:\n",
    "    \"\"\"\n",
    "    Bricht das Programm ab, wenn der Prompt zu viele Tokens enth√§lt.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    token_count = len(encoding.encode(prompt))\n",
    "    \n",
    "    if token_count > max_tokens:\n",
    "        raise ValueError(f\"‚ùå Prompt ist zu lang ({token_count} Tokens). Maximal erlaubt: {max_tokens}.\\nBitte k√ºrzen oder aufteilen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e0d341-cae6-4c96-b273-29be99738660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rows_with_header_into_token_batches(df: pd.DataFrame, max_tokens_per_batch: int, model: str = \"gpt-4\") -> list[str]:\n",
    "    \"\"\"\n",
    "    Zerlegt den DataFrame in Pakete aus Zeilen (jede Zeile = Eintrag),\n",
    "    wobei jeder Block den Header (Spaltennamen) enth√§lt.\n",
    "    Gibt eine Liste von Textbl√∂cken zur√ºck, die unter dem Tokenlimit bleiben.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    batches = []\n",
    "    current_rows = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    # Header vorbereiten\n",
    "    header_text = \"\\n\".join([f\"{col}:\" for col in df.columns]) + \"\\n\\n\"\n",
    "    header_token_count = len(encoding.encode(header_text))\n",
    "\n",
    "    if header_token_count >= max_tokens_per_batch:\n",
    "        raise ValueError(\"‚ùå Der Header allein √ºberschreitet das Tokenlimit.\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Zeile als formatierten Text\n",
    "        row_text = \"\\n\".join([f\"{col}: {val}\" for col, val in row.items() if pd.notna(val)]) + \"\\n\\n\"\n",
    "        row_token_count = len(encoding.encode(row_text))\n",
    "\n",
    "        if row_token_count >= max_tokens_per_batch - header_token_count:\n",
    "            raise ValueError(\"‚ùå Einzelne Zeile + Header √ºberschreiten das Tokenlimit.\")\n",
    "\n",
    "        if current_token_count + row_token_count + header_token_count > max_tokens_per_batch:\n",
    "            # Block abschlie√üen\n",
    "            batch_text = header_text + \"\".join(current_rows)\n",
    "            batches.append(batch_text.strip())\n",
    "            # neuen Block beginnen\n",
    "            current_rows = [row_text]\n",
    "            current_token_count = row_token_count\n",
    "        else:\n",
    "            current_rows.append(row_text)\n",
    "            current_token_count += row_token_count\n",
    "\n",
    "    # letzten Block hinzuf√ºgen\n",
    "    if current_rows:\n",
    "        batch_text = header_text + \"\".join(current_rows)\n",
    "        batches.append(batch_text.strip())\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a9feb7c-b1bd-44dc-8d53-a81322c5f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_chatgpt_single_prompt(\n",
    "    prompt: str,\n",
    "    lan: str = 'de',\n",
    "    model: str = \"gpt-4\",\n",
    "    temperature: float = 0.2,\n",
    "    max_retries: int = 2,\n",
    "    delay_seconds: float = 2.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Sendet einen einzelnen Prompt an die OpenAI-API (ab openai>=1.0.0) und gibt die Antwort als String zur√ºck.\n",
    "    \"\"\"\n",
    "    client = OpenAI()  # Liest den API-Key automatisch aus der Umgebungsvariable OPENAI_API_KEY\n",
    "    if lan == 'de':\n",
    "        system_instruction = \"Du bist ein hilfreicher Assistent. Antworte bitte auf Deutsch.\"\n",
    "    elif lan == 'en':\n",
    "        system_instruction = \"You are a helpful assistant. Please answer in English.\"\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(\"üì§ Sende Anfrage an ChatGPT‚Ä¶\")\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_instruction},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            print(\"‚úÖ Antwort erhalten.\")\n",
    "            return content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Fehler bei der Anfrage (Versuch {attempt}): {e}\")\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(delay_seconds)\n",
    "            else:\n",
    "                return f\"‚ùå Fehler nach {max_retries} Versuchen: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a779655-910f-4f3b-9c85-cb5b4ce21c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_profil(pdf_text, df, language='de'):\n",
    "    \n",
    "    prompt_text_job = get_prompt(\"reduce_pdf\", language=\"de\")\n",
    "    filled_prompt_job = prompt_text_job.replace(\"{pdf_text}\", pdf_text)\n",
    "    \n",
    "    validate_prompt_length(filled_prompt_job, model=\"gpt-4\", max_tokens=8192)\n",
    "\n",
    "    short_promt_job = ask_chatgpt_single_prompt(filled_prompt_job)\n",
    "\n",
    "    prompt_text_df = get_prompt(\"select_experiences\", language=\"de\")\n",
    "    filled_prompt_df = prompt_text_df.replace(\"{pdf_text}\", short_promt_job)\n",
    "    \n",
    "    max_tokens = 8192\n",
    "    limit_df = max_tokens - count_tokens(filled_prompt_df)\n",
    "\n",
    "    batches = split_rows_with_header_into_token_batches(df, limit_df)\n",
    "\n",
    "    while len(batches) > 1:\n",
    "        profils = []\n",
    "        for batch in batches:\n",
    "            filled_prompt_df = filled_prompt_df.replace(\"{csv_als_text}\", batch)\n",
    "            validate_prompt_length(filled_prompt_df, model=\"gpt-4\", max_tokens=8192)\n",
    "            profil = ask_chatgpt_single_prompt(filled_prompt_job)\n",
    "            profils.append(profil)\n",
    "        batches = split_rows_with_header_into_token_batches(profils, limit_df) # problem beheben \n",
    "    if len(batches) == 1: \n",
    "        filled_prompt_df = filled_prompt_df.replace(\"{csv_als_text}\", batches[0])\n",
    "    output = 'output'\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5ac6792-1733-4aca-be49-42cbe0ff2221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Sende Anfrage an ChatGPT‚Ä¶\n",
      "‚ö†Ô∏è Fehler bei der Anfrage (Versuch 1): Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "üì§ Sende Anfrage an ChatGPT‚Ä¶\n",
      "‚ö†Ô∏è Fehler bei der Anfrage (Versuch 2): Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "üì§ Sende Anfrage an ChatGPT‚Ä¶\n",
      "‚ö†Ô∏è Fehler bei der Anfrage (Versuch 1): Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "üì§ Sende Anfrage an ChatGPT‚Ä¶\n",
      "‚ö†Ô∏è Fehler bei der Anfrage (Versuch 2): Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "1\n",
      "üì§ Sende Anfrage an ChatGPT‚Ä¶\n",
      "‚ö†Ô∏è Fehler bei der Anfrage (Versuch 1): Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "üì§ Sende Anfrage an ChatGPT‚Ä¶\n",
      "‚ö†Ô∏è Fehler bei der Anfrage (Versuch 2): Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "1\n",
      "üì§ Sende Anfrage an ChatGPT‚Ä¶\n",
      "‚ö†Ô∏è Fehler bei der Anfrage (Versuch 1): Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "üì§ Sende Anfrage an ChatGPT‚Ä¶\n",
      "‚ö†Ô∏è Fehler bei der Anfrage (Versuch 2): Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "1\n",
      "üì§ Sende Anfrage an ChatGPT‚Ä¶\n",
      "‚ö†Ô∏è Fehler bei der Anfrage (Versuch 1): Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "üì§ Sende Anfrage an ChatGPT‚Ä¶\n",
      "‚ö†Ô∏è Fehler bei der Anfrage (Versuch 2): Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# filtern des dataframe\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcreate_profil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mcreate_profil\u001b[39m\u001b[34m(pdf_text, df, language)\u001b[39m\n\u001b[32m     24\u001b[39m         profils.append(profil)\n\u001b[32m     25\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     batches = \u001b[43msplit_rows_with_header_into_token_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofils\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) == \u001b[32m1\u001b[39m: \n\u001b[32m     28\u001b[39m     filled_prompt_df = filled_prompt_df.replace(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{csv_als_text}\u001b[39;00m\u001b[33m\"\u001b[39m, batches[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36msplit_rows_with_header_into_token_batches\u001b[39m\u001b[34m(df, max_tokens_per_batch, model)\u001b[39m\n\u001b[32m     10\u001b[39m current_token_count = \u001b[32m0\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Header vorbereiten\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m header_text = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m]) + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m header_token_count = \u001b[38;5;28mlen\u001b[39m(encoding.encode(header_text))\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m header_token_count >= max_tokens_per_batch:\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# filtern des dataframe\n",
    "create_profil(pdf_text, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a004f36-9a46-4e38-8f58-d7d255ede0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e76b94-4d07-49c9-98ad-fcd8ba65e0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2275a46-87ff-45ce-a417-20aa2fc8009a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb417e0f-ee7c-4fb8-a2be-b519008cc26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03d2b9-f00a-459f-b416-c0de1242470b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
