### Einlesen CSV

import os
import pandas as pd
import re
import pdfplumber
import hashlib

try:
    from app.openai_client import ask_chatgpt_single_prompt, validate_prompt_length
except ImportError:
    from openai_client import ask_chatgpt_single_prompt, validate_prompt_length

CACHE_DIR = "./data/cache"

def find_csv_and_pdf_files(path: str) -> tuple[list[str], list[str]]:
    """
    Sucht nach allen CSV- und PDF-Dateien in einem Verzeichnis und gibt deren Pfade zurÃ¼ck.
    Gibt zusÃ¤tzlich Feedback zur Anzahl der gefundenen Dateien.
    """
    if not os.path.isdir(path):
        raise NotADirectoryError(f"âŒ Der Pfad ist kein gÃ¼ltiges Verzeichnis: {path}")

    files = os.listdir(path)
    
    if not files:
        raise FileNotFoundError("âŒ Keine Dateien im angegebenen Verzeichnis gefunden.")

    csv_files = [os.path.join(path, f) for f in files if f.lower().endswith(".csv")]
    pdf_files = [os.path.join(path, f) for f in files if f.lower().endswith(".pdf")]

    print(f"ğŸ“„ Gefundene CSV-Dateien: {len(csv_files)}")
    print(f"ğŸ“„ Gefundene PDF-Dateien: {len(pdf_files)}")

    if not pdf_files:
        raise FileNotFoundError("âŒ Keine PDF-Datei gefunden. Bitte stelle sicher, dass mindestens eine PDF im Ordner liegt.")

    return csv_files, pdf_files

def detect_separator(filepath: str) -> str:
    """
    Erkennt automatisch das Trennzeichen in der ersten Zeile der Datei.
    UnterstÃ¼tzt ',', ';' und Tabulator ('\\t').
    """
    with open(filepath, encoding="utf-8") as f:
        first_line = f.readline()

    # ZÃ¤hle Vorkommen mÃ¶glicher Trenner
    candidates = {",": first_line.count(","), ";": first_line.count(";"), "\t": first_line.count("\t")}

    # WÃ¤hle den Trenner mit den meisten Vorkommen
    best = max(candidates, key=candidates.get)

    # Warnung, falls alles gleich 0 ist (z.â€¯B. eine leere oder ungewÃ¶hnliche Datei)
    if candidates[best] == 0:
        print("âš ï¸ Kein gÃ¤ngiges Trennzeichen erkannt, Standardwert ',' wird verwendet.")
        return ","

    return best

def parse_date_string(date_str: str) -> pd.Timestamp | None:
    """Konvertiert Strings wie '2010', 'heute', 'today' in ein Datum, sonst None."""
    date_str = date_str.strip().lower()
    today = pd.Timestamp.today().normalize()

    if date_str in {"heute", "today"}:
        return today

    try:
        return pd.to_datetime(date_str, errors="raise", dayfirst=True)
    except Exception:
        return None
    
def parse_period_string(entry: str) -> tuple[pd.Timestamp, pd.Timestamp] | None:
    """
    Erkennt entweder:
    - Einzelzeitpunkt wie '2010', 'heute' â†’ Start = End
    - Zeitraum wie '2010 - 2015' â†’ Start, End
    Gibt: (start, end) oder None
    """
    entry = entry.strip()

    # Trenne Bereich
    split_match = re.split(r"\s*[-â€“bis]+\s*", entry, maxsplit=1)
    if len(split_match) == 1:
        date = parse_date_string(split_match[0])
        return (date, date) if date else None
    elif len(split_match) == 2:
        start = parse_date_string(split_match[0])
        end = parse_date_string(split_match[1])
        return (start, end) if start and end else None
    return None

def quality_check(df: pd.DataFrame, time_column: str = 'Timeline') -> bool:
    """
    PrÃ¼ft ob:
    - Zeitspalte vorhanden
    - mind. 4 gÃ¼ltige EintrÃ¤ge (einzelne Zeitpunkte oder ZeitrÃ¤ume)
    - Gesamtzeitraum >= 1 Jahr
    """
    if time_column not in df.columns:
        print(f"âŒ Zeitspalte '{time_column}' nicht gefunden.")
        return False

    if len(df) < 4:
        print(f"âŒ Der DataFrame enthÃ¤lt nur {len(df)} Zeilen (mindestens 4 erforderlich).")
        return False

    df = df.copy()
    periods = df[time_column].astype(str).apply(parse_period_string)
    valid_periods = [p for p in periods if p is not None]

    if len(valid_periods) < 4:
        print(f"âŒ Nur {len(valid_periods)} gÃ¼ltige Zeitangaben erkannt (mindestens 4 erforderlich).")
        return False

    starts, ends = zip(*valid_periods)
    overall_start = min(starts)
    overall_end = max(ends)
    duration = overall_end - overall_start

    if duration < pd.Timedelta(days=365):
        print(f"âŒ Zeitraum ist zu kurz: nur {duration.days} Tage (mind. 1 Jahr erforderlich).")
        return False

    print(f"âœ… Es wurden {len(df)} Berufserfahrungen erkannt. Der Zeitraum erstreckt sich von {overall_start.strftime('%B %Y')} bis {overall_end.strftime('%B %Y')} ({(overall_end - overall_start).days} Tage).")
    print("âœ… QualitÃ¤tsprÃ¼fung bestanden.")
    return True

def load_resume_data(filepath: str) -> pd.DataFrame:
    
    sep = detect_separator(filepath)

    try:
        # Korrektur: Header ist in Zeile 0
        df = pd.read_csv(filepath, sep=sep, header=0)
    except Exception as e:
        raise ValueError(f"âŒ Fehler beim Einlesen der CSV: {e}")

    # Leere Spalten entfernen
    df = df.drop(columns=[col for col in df.columns if col.startswith("Unnamed")], errors="ignore")

    # Leere Zeilen entfernen
    df = df.dropna(how="all")

    # Whitespace aus Spaltennamen entfernen
    df.columns = [col.strip() for col in df.columns]

    # ToDo: Hier noch einen QualitÃ¤tscheck einfÃ¼gen. z.B. Datensatz nach Relevanz filtern --> vorgeschriebene Spalten oder mit Chatgpt Spaltennamen nach Relevanz auswerten
    quality_check(df) 
    return df

### Einlesen PDF

def normalize_pdf_text(text: str) -> str:
    """
    Bereinigt gestreckten, duplizierten oder zerschossenen Text aus PDF-Scans.
    - Fasst Buchstabenwiederholungen wie 'TTTeeecccchhhh' zu 'Tech' zusammen
    - Entfernt mehrfach wiederholte WÃ¶rter oder BlÃ¶cke
    - Vereinheitlicht Leerzeichen
    """

    # 1. Einzelne Buchstaben-Wiederholungen reduzieren (z.B. SSSoooffftttwwwaaarrreee â†’ Software)
    text = re.sub(r'(.)\1{2,}', r'\1', text)

    # 2. Ãœberlange Wortwiederholungen zusammenfassen
    text = re.sub(r'\b(\w{3,})\s+\1\b', r'\1', text)

    # 3. Whitespace bereinigen
    text = re.sub(r'\s+', ' ', text)

    # 4. Erste Buchstaben wieder groÃŸ schreiben (optional)
    text = re.sub(r'(^|\.\s+)([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)

    # 5. Trim
    text = text.strip()

    return text

def extract_clean_text_from_pdf(pdf_path: str) -> str:
    """
    Extrahiert und bereinigt den Text aus einer PDF-Datei mit pdfplumber.
    
    Vorteile gegenÃ¼ber PyPDF2:
    - Bessere Textstruktur
    - Weniger Layout-MÃ¼ll
    - Keine zerrissenen WÃ¶rter oder Duplikate
    
    RÃ¼ckgabe:
        str: Bereinigter Text der PDF
    """
    try:
        with pdfplumber.open(pdf_path) as pdf:
            raw_text = " ".join(page.extract_text() or "" for page in pdf.pages)
    except Exception as e:
        raise ValueError(f"âŒ Fehler beim Ã–ffnen oder Lesen der PDF-Datei: {e}")

    # Text bereinigen
    cleaned_text = normalize_pdf_text(raw_text)

    if not cleaned_text.strip():
        raise ValueError("âŒ Es konnte kein sinnvoller Text aus der PDF extrahiert werden.")
    
    print("âœ… Die Stellenauschreibung wurde erkannt.")

    return cleaned_text

def _get_cache_path(key: str) -> str:
        return os.path.join(CACHE_DIR, f"{key}.txt")

def load_cached_reduction(key: str) -> str | None:
        os.makedirs(CACHE_DIR, exist_ok=True)
        path = _get_cache_path(key)
        if os.path.exists(path):
            print("ğŸ”„ Verwende gecachte Reduktion.")
            return open(path, "r", encoding="utf-8").read()
        return None

def save_cached_reduction(key: str, text: str):
        os.makedirs(CACHE_DIR, exist_ok=True)
        with open(_get_cache_path(key), "w", encoding="utf-8") as f:
            f.write(text)

def make_key_from_file(pdf_path: str) -> str:
    buf = open(pdf_path, "rb").read()
    return hashlib.sha256(buf).hexdigest()

def reduce_pdf_to_essentials(text: str, pdf_path : str, cache_key: str) -> str:
    """
    Reduziert den PDF-Text auf das Wesentliche via ChatGPT, mit einfachem Cache.
    """
    # Ein Key: entweder aus Dateiname oder Hash des Inhalts
    
    prompt = (
        "Hier ist der Text einer Stellenanzeige. Fasse die Anzeige in 5â€“10 Bullet Points zusammen "
        "mit Fokus auf die wichtigsten Anforderungen, Aufgaben und Qualifikationen. "
        "Lass irrelevante Informationen weg (z.â€¯B. Selbstbeschreibungen des Unternehmens):\n\n"
        f"{text}"
    )

    validate_prompt_length(prompt)
    reduced = ask_chatgpt_single_prompt(prompt)

    save_cached_reduction(cache_key, reduced)

    print("âœ… PDF-Inhalt wurde erfolgreich reduziert.")
    return reduced

def process_pdf(pdf_path: str) -> str:
    """
    Verwendet zunÃ¤chst den gecachten Digest (Hash) der PDF, falls vorhanden.
    Nur wenn noch keine Zusammenfassung existiert,
    wird die PDF eingelesen, reduziert und gespeichert.
    """
    # 1) Key bestimmen anhand des Datei-Hashes
    key = make_key_from_file(pdf_path)

    # 2) Cache prÃ¼fen
    cached = load_cached_reduction(key)
    if cached:
        print(f"ğŸ”„ Verwende gecachte Reduktion fÃ¼r SchlÃ¼ssel {key}.")
        return cached

    # 3) PDF neu verarbeiten und Zusammenfassung speichern
    pdf_text     = extract_clean_text_from_pdf(pdf_path)
    reduced_text = reduce_pdf_to_essentials(pdf_text, pdf_path, cache_key=key)
    return reduced_text