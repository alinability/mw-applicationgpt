### Einlesen CSV

import os
import pandas as pd
import re
import pdfplumber
import hashlib
import os
import re
import pdfplumber
import ocrmypdf
import tempfile
import yaml
from pathlib import Path
from collections import OrderedDict

PROMPTS = yaml.safe_load(
    (Path(__file__).parent / "prompts.yml")
    .read_text(encoding="utf-8")
)

try:
    from app.openai_client import ask_chatgpt_single_prompt, validate_prompt_length
    from app.prompt_utils import count_tokens, chunk_text_by_tokens
except ImportError:
    from openai_client import ask_chatgpt_single_prompt, validate_prompt_length
    from prompt_utils import count_tokens, chunk_text_by_tokens

CACHE_DIR = "./data/cache"

def find_csv_and_pdf_files(path: str) -> tuple[list[str], list[str]]:
    """
    Sucht nach allen CSV- und PDF-Dateien in einem Verzeichnis und gibt deren Pfade zur√ºck.
    Gibt zus√§tzlich Feedback zur Anzahl der gefundenen Dateien.
    """
    if not os.path.isdir(path):
        raise NotADirectoryError(f"‚ùå Der Pfad ist kein g√ºltiges Verzeichnis: {path}")

    files = os.listdir(path)
    
    if not files:
        raise FileNotFoundError("‚ùå Keine Dateien im angegebenen Verzeichnis gefunden.")

    csv_files = [os.path.join(path, f) for f in files if f.lower().endswith(".csv")]
    pdf_files = [os.path.join(path, f) for f in files if f.lower().endswith(".pdf")]

    print(f"üìÑ Gefundene CSV-Dateien: {len(csv_files)}")
    print(f"üìÑ Gefundene PDF-Dateien: {len(pdf_files)}")

    if not pdf_files:
        raise FileNotFoundError("‚ùå Keine PDF-Datei gefunden. Bitte stelle sicher, dass mindestens eine PDF im Ordner liegt.")

    return csv_files, pdf_files

def detect_separator(filepath: str) -> str:
    """
    Erkennt automatisch das Trennzeichen in der ersten Zeile der Datei.
    Unterst√ºtzt ',', ';' und Tabulator ('\\t').
    """
    with open(filepath, encoding="utf-8") as f:
        first_line = f.readline()

    # Z√§hle Vorkommen m√∂glicher Trenner
    candidates = {",": first_line.count(","), ";": first_line.count(";"), "\t": first_line.count("\t")}

    # W√§hle den Trenner mit den meisten Vorkommen
    best = max(candidates, key=candidates.get)

    # Warnung, falls alles gleich 0 ist (z.‚ÄØB. eine leere oder ungew√∂hnliche Datei)
    if candidates[best] == 0:
        print("‚ö†Ô∏è Kein g√§ngiges Trennzeichen erkannt, Standardwert ',' wird verwendet.")
        return ","

    return best

def parse_date_string(date_str: str) -> pd.Timestamp | None:
    """Konvertiert Strings wie '2010', 'heute', 'today' in ein Datum, sonst None."""
    date_str = date_str.strip().lower()
    today = pd.Timestamp.today().normalize()

    if date_str in {"heute", "today"}:
        return today

    try:
        return pd.to_datetime(date_str, errors="raise", dayfirst=True)
    except Exception:
        return None
    
def parse_period_string(entry: str) -> tuple[pd.Timestamp, pd.Timestamp] | None:
    """
    Erkennt entweder:
    - Einzelzeitpunkt wie '2010', 'heute' ‚Üí Start = End
    - Zeitraum wie '2010 - 2015' ‚Üí Start, End
    Gibt: (start, end) oder None
    """
    entry = entry.strip()

    # Trenne Bereich
    split_match = re.split(r"\s*[-‚Äìbis]+\s*", entry, maxsplit=1)
    if len(split_match) == 1:
        date = parse_date_string(split_match[0])
        return (date, date) if date else None
    elif len(split_match) == 2:
        start = parse_date_string(split_match[0])
        end = parse_date_string(split_match[1])
        return (start, end) if start and end else None
    return None

def quality_check(df: pd.DataFrame, time_column: str = 'Timeline') -> bool:
    """
    Pr√ºft ob:
    - Zeitspalte vorhanden
    - mind. 4 g√ºltige Eintr√§ge (einzelne Zeitpunkte oder Zeitr√§ume)
    - Gesamtzeitraum >= 1 Jahr
    """
    if time_column not in df.columns:
        print(f"‚ùå Zeitspalte '{time_column}' nicht gefunden.")
        return False

    if len(df) < 4:
        print(f"‚ùå Der DataFrame enth√§lt nur {len(df)} Zeilen (mindestens 4 erforderlich).")
        return False

    df = df.copy()
    periods = df[time_column].astype(str).apply(parse_period_string)
    valid_periods = [p for p in periods if p is not None]

    if len(valid_periods) < 4:
        print(f"‚ùå Nur {len(valid_periods)} g√ºltige Zeitangaben erkannt (mindestens 4 erforderlich).")
        return False

    starts, ends = zip(*valid_periods)
    overall_start = min(starts)
    overall_end = max(ends)
    duration = overall_end - overall_start

    if duration < pd.Timedelta(days=365):
        print(f"‚ùå Zeitraum ist zu kurz: nur {duration.days} Tage (mind. 1 Jahr erforderlich).")
        return False

    print(f"‚úÖ Es wurden {len(df)} Berufserfahrungen erkannt. Der Zeitraum erstreckt sich von {overall_start.strftime('%B %Y')} bis {overall_end.strftime('%B %Y')} ({(overall_end - overall_start).days} Tage).")
    print("‚úÖ Qualit√§tspr√ºfung bestanden.")
    return True

def load_resume_data(filepath: str) -> pd.DataFrame:
    
    sep = detect_separator(filepath)

    try:
        # Korrektur: Header ist in Zeile 0
        df = pd.read_csv(filepath, sep=sep, header=0)
    except Exception as e:
        raise ValueError(f"‚ùå Fehler beim Einlesen der CSV: {e}")

    # Leere Spalten entfernen
    df = df.drop(columns=[col for col in df.columns if col.startswith("Unnamed")], errors="ignore")

    # Leere Zeilen entfernen
    df = df.dropna(how="all")

    # Whitespace aus Spaltennamen entfernen
    df.columns = [col.strip() for col in df.columns]

    # ToDo: Hier noch einen Qualit√§tscheck einf√ºgen. z.B. Datensatz nach Relevanz filtern --> vorgeschriebene Spalten oder mit Chatgpt Spaltennamen nach Relevanz auswerten
    quality_check(df) 
    return df

### Einlesen PDF

def normalize_pdf_text(text: str) -> str:
    """
    Bereinigt gestreckten, duplizierten oder zerschossenen Text aus PDF-Scans.
    - Fasst Buchstabenwiederholungen wie 'TTTeeecccchhhh' zu 'Tech' zusammen
    - Entfernt mehrfach wiederholte W√∂rter oder Bl√∂cke
    - Vereinheitlicht Leerzeichen
    """

    # 1. Einzelne Buchstaben-Wiederholungen reduzieren (z.B. SSSoooffftttwwwaaarrreee ‚Üí Software)
    text = re.sub(r'(.)\1{2,}', r'\1', text)

    # 2. √úberlange Wortwiederholungen zusammenfassen
    text = re.sub(r'\b(\w{3,})\s+\1\b', r'\1', text)

    # 3. Whitespace bereinigen
    text = re.sub(r'\s+', ' ', text)

    # 4. Erste Buchstaben wieder gro√ü schreiben (optional)
    text = re.sub(r'(^|\.\s+)([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)

    # 5. Trim
    text = text.strip()

    return text

def extract_clean_text_from_pdf(pdf_path: str) -> str:
    """
    Extrahiert und bereinigt den Text aus einer PDF-Datei mit pdfplumber.
    Falls dabei kein Text herauskommt, wird OCR nachgeladen und erneut extrahiert.
    
    Vorteile gegen√ºber PyPDF2:
    - Bessere Textstruktur
    - Weniger Layout-M√ºll
    - Keine zerrissenen W√∂rter oder Duplikate
    
    R√ºckgabe:
        str: Bereinigter Text der PDF
    """
    def _read_pdf(path: str) -> str:
        with pdfplumber.open(path) as pdf:
            return " ".join(page.extract_text() or "" for page in pdf.pages)

    # 1) Erster Versuch ohne OCR
    try:
        raw_text = _read_pdf(pdf_path)
    except Exception as e:
        raise ValueError(f"‚ùå Fehler beim √ñffnen oder Lesen der PDF-Datei: {e}")

    cleaned_text = normalize_pdf_text(raw_text)

    # 2) Falls kein sinnvoller Text, OCR-Fallback
    ocr_pdf_path = False
    pattern = re.compile(r'^(?:\b\w+\b\W+){49}\b\w+\b', flags=re.UNICODE)
    if not bool(pattern.match(cleaned_text.strip())):
    #if not cleaned_text.strip():
        print("‚ÑπÔ∏è Kein Text gefunden, f√ºhre OCR nach ...")
        fd, ocr_pdf_path = tempfile.mkstemp(suffix=".pdf")
        os.close(fd)
        try:
            # force_ocr sorgt daf√ºr, dass auch textbasierte PDFs verarbeitet werden
           ocrmypdf.ocr(
                pdf_path,
                ocr_pdf_path,
                output_type="pdf",
                force_ocr=True,
                skip_text=False,
                redo_ocr=False
            )
           raw_text = _read_pdf(ocr_pdf_path)
           cleaned_text = normalize_pdf_text(raw_text)
           if cleaned_text.strip():
                print("‚úÖ OCR erfolgreich, Text extrahiert.")
           else:
               print("‚ö†Ô∏è Auch nach OCR konnte kein Text extrahiert werden.")

        except Exception as oe:
            raise ValueError(f"‚ùå OCR-Fehler: {oe}")

    if not cleaned_text.strip():
        raise ValueError("‚ùå Es konnte kein sinnvoller Text aus der PDF extrahiert werden.")
    
    print("‚úÖ Die Stellenausschreibung wurde erkannt.")
    return cleaned_text, ocr_pdf_path

def _get_cache_path(key: str) -> str:
        return os.path.join(CACHE_DIR, f"{key}.txt")

def load_cached_reduction(key: str) -> str | None:
        os.makedirs(CACHE_DIR, exist_ok=True)
        path = _get_cache_path(key)
        if os.path.exists(path):
            print("üîÑ Verwende gecachte Reduktion.")
            return open(path, "r", encoding="utf-8").read()
        return None

def save_cached_reduction(key: str, text: str):
        os.makedirs(CACHE_DIR, exist_ok=True)
        with open(_get_cache_path(key), "w", encoding="utf-8") as f:
            f.write(text)

def make_key_from_file(pdf_path: str) -> str:
    buf = open(pdf_path, "rb").read()
    return hashlib.sha256(buf).hexdigest()

def remove_stopwords(text: str) -> str:
    GERMAN_STOP_WORDS = {"der","die","das","und","oder","ein","eine","zu","von"}
    words = text.split()
    filtered = [w for w in words if w.lower() not in GERMAN_STOP_WORDS]
    return " ".join(filtered)

def reduce_pdf_to_essentials(text: str, pdf_path: str, cache_key: str) -> str:
    """
    Reduziert den PDF-Text auf das Wesentliche via ChatGPT, mit einfachem Cache.
    Nutzt die Prompt-Templates aus PROMPTS (geladen aus prompts.yml).
    """
     # Gesamtes Dokument in einem Prompt reduzieren
    template = PROMPTS["reduce_pdf"]
    filled = template.format(text=text)
    # Prompt-L√§nge validieren
    if not validate_prompt_length(filled):

        # Wenn wir eine Liste von Kapiteln haben, jeweils einzeln reduzieren
        if isinstance(text, list):
            reduced_final = ""
            for chapter in text:
                # Prompt-Template holen und f√ºllen
                template = PROMPTS["reduce_pdf"]
                filled = template.format(text=chapter)
                # L√§nge pr√ºfen und senden
                if not validate_prompt_length(filled):
                    raise ValueError("Kapitel-Prompt zu lang")
                chunk = ask_chatgpt_single_prompt(filled)
                reduced_final += chunk + "\n"
            text = reduced_final.strip()
            template = PROMPTS["reduce_pdf"]
            filled = template.format(text=text)
        else:
            # ggf. Stopw√∂rter entfernen und neu f√ºllen
            text_slim = remove_stopwords(text)
            filled = template.format(text=text_slim)
            if not validate_prompt_length(filled):
                # ggf. in Chunks aufteilen
                parts = chunk_text_by_tokens(text)
                reduced_parts = []
                for p in parts:
                    tmp = PROMPTS["reduce_pdf"].format(text=p)
                    if not validate_prompt_length(tmp):
                        raise ValueError("Chunk-Prompt zu lang")
                    reduced_parts.append(ask_chatgpt_single_prompt(tmp))
                reduced = "\n".join(reduced_parts)
                save_cached_reduction(cache_key, reduced)
                print("‚úÖ PDF-Inhalt wurde erfolgreich in Chunks reduziert.")
                return reduced

    # 3) Einfache Reduktion
    reduced = ask_chatgpt_single_prompt(filled)
    save_cached_reduction(cache_key, reduced)
    print("‚úÖ PDF-Inhalt wurde erfolgreich reduziert.")
    return reduced

def get_pdf_page_count(pdf_path: str) -> int:
    """Z√§hlt die Seiten einer PDF."""
    with pdfplumber.open(pdf_path) as pdf:
        return len(pdf.pages)

def extract_headings_from_pdf(pdf_path: str) -> list[str]:
    """
    Liest jede Seite mit pdfplumber ein, splittet sie in Zeilen
    und sammelt alle Zeilen, die mit ‚ÄûZahl. Text‚Äú beginnen.
    Dubletten (gleiche Kapitelnummer) werden entfernt,
    jeweils die letzte Vorkommen behalten.
    """
    heads: list[tuple[str,str]] = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            for line in text.split("\n"):
                clean = line.strip()
                # Muster: ganze Kapitelnummer (z.B. 3 oder 10.2.1), Punkt, mindestens ein Leerzeichen, dann Text (mit Buchstabe)
                m = re.match(r'^(\d+(?:\.)*\.\s+[A-Za-z√Ñ√ñ√ú√§√∂√º,\s-]+)(\d?)', clean)
                if not m:
                    continue
                title, page = m.groups()
                if page != '':
                    continue
                heads.append(title)
    return list(heads)

def select_best_heading(headings: list[str]) -> list[str]:
    """
    Fragt ChatGPT, welche Heading(s) am relevantesten f√ºr die Stellenanzeige sind.
    Liefert eine ganz normale Python-Liste von Kapitel-Titeln zur√ºck.
    """
    # Inhaltsverzeichnis als Bullet-Point-Liste f√ºr den Prompt aufbauen
    toc = "\n".join(f"- {h}" for h in headings)

    # Prompt-Template aus YAML holen und with toc bef√ºllen
    template: str = PROMPTS["select_heading"]
    prompt = template.format(toc=toc)

    # API-Call
    raw = ask_chatgpt_single_prompt(prompt, "gpt-4o").strip()

    # Antwort parsen
    lines = [line.strip() for line in raw.splitlines() if line.strip()]
    result: list[str] = []
    for line in lines:
        if line.startswith("-"):
            result.append(line.lstrip("- ").strip())
        else:
            result.append(line)
    return result

def extract_chapter_text(full_text: str,
                         chosen: str | list[str],
                         all_headings: list[str]) -> list[str]:
    """
    Schneidet aus full_text f√ºr jede √úberschrift in `chosen` den kompletten Abschnitt
    (√úberschrift + Inhalt bis zur n√§chsten √úberschrift oder Dokumentende) heraus.
    Dabei wird bei mehrfachen Vorkommen einer √úberschrift immer der **zweite** Fund 
    (z.B. im Flie√ütext statt im Inhaltsverzeichnis) als Startposition genutzt.
    Gibt immer eine Liste von Strings zur√ºck, selbst bei nur einer √úberschrift.
    """

    # 1) Finde alle tats√§chlichen Vorkommen der √úberschriften mit ihrer Position
    matches: list[tuple[int, str]] = []
    for h in all_headings:
        # suche alle Vorkommen in eigenen Zeilen
        pattern = rf'{re.escape(h)}'
        found = list(re.finditer(pattern, full_text))
        if not found:
            continue

        # nimm das zweite Vorkommen, falls vorhanden, sonst das erste
        m = found[1] if len(found) > 1 else found[0]
        matches.append((m.start(), h))

    # falls keine gefunden, leere Liste
    if not matches:
        return []

    # 2) Sortiere nach Position im Text
    matches.sort(key=lambda x: x[0])

    # 3) Bestimme f√ºr jede gefundene √úberschrift den entsprechenden Abschnitt
    sections: dict[str, str] = {}
    for idx, (pos, h) in enumerate(matches):
        start = pos
        end = matches[idx + 1][0] if idx + 1 < len(matches) else len(full_text)
        sections[h] = full_text[start:end].strip()
    # 4) Baue R√ºckgabeliste f√ºr gew√§hlte Heading(s)
    chosen_list = chosen if isinstance(chosen, (list, tuple)) else [chosen]
    result: list[str] = []
    for h in chosen_list:
        if h in sections:
            result.append(sections[h])
    return result

def process_pdf(pdf_path: str) -> str:
    """
    - Nutzt Cache, sofern vorhanden.
    - Liest PDF, falls ‚â§3 Seiten: Volltext ‚Üí reduce.
    - Falls >3 Seiten: sucht Kapitel, fragt ChatGPT, extrahiert bestes Kapitel ‚Üí reduce.
    """
    # 1) Cache-Key pr√ºfen
    key = make_key_from_file(pdf_path)
    cached = load_cached_reduction(key)
    if cached is not None:
        return cached

    # 2) PDF √∂ffnen und Volltext sammeln
    full_text, ocr_pdf_path = extract_clean_text_from_pdf(pdf_path)
    token_count = count_tokens(full_text)
    
    # 3) Bei kurzen PDFs: direkt reduzieren
    if token_count <= 2000:
        reduced = reduce_pdf_to_essentials(full_text, pdf_path, cache_key=key)
        save_cached_reduction(key, reduced)
        return reduced

    # 4) Bei langen PDFs: Kapitel-Handling
    if ocr_pdf_path:
        headings = extract_headings_from_pdf(ocr_pdf_path)
    else:
        headings = extract_headings_from_pdf(pdf_path)

    if not headings:
        # Falls keine Kapitel gefunden: wie Kurz-PDF behandeln
        reduced = reduce_pdf_to_essentials(full_text, pdf_path, cache_key=key)
    
    if headings:
        best = select_best_heading(headings)
        #print(f"‚ÑπÔ∏è W√§hle Kapitel ‚Äû{best}‚Äú f√ºr Reduktion.")
        full_text = extract_chapter_text(full_text, best, headings)
        reduced = reduce_pdf_to_essentials(full_text, pdf_path, cache_key=key)
    # 5) Dieses Kapitel reduzieren und cachen
    save_cached_reduction(key, reduced)
    return reduced

