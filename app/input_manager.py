### Einlesen CSV

import os
import pandas as pd
import re
import pdfplumber
import hashlib
import os
import re
import pdfplumber
import ocrmypdf
import tempfile

try:
    from app.openai_client import ask_chatgpt_single_prompt, validate_prompt_length
    from app.prompt_utils import count_tokens, chunk_text_by_tokens
except ImportError:
    from openai_client import ask_chatgpt_single_prompt, validate_prompt_length
    from prompt_utils import count_tokens, chunk_text_by_tokens

CACHE_DIR = "./data/cache"

def find_csv_and_pdf_files(path: str) -> tuple[list[str], list[str]]:
    """
    Sucht nach allen CSV- und PDF-Dateien in einem Verzeichnis und gibt deren Pfade zur√ºck.
    Gibt zus√§tzlich Feedback zur Anzahl der gefundenen Dateien.
    """
    if not os.path.isdir(path):
        raise NotADirectoryError(f"‚ùå Der Pfad ist kein g√ºltiges Verzeichnis: {path}")

    files = os.listdir(path)
    
    if not files:
        raise FileNotFoundError("‚ùå Keine Dateien im angegebenen Verzeichnis gefunden.")

    csv_files = [os.path.join(path, f) for f in files if f.lower().endswith(".csv")]
    pdf_files = [os.path.join(path, f) for f in files if f.lower().endswith(".pdf")]

    print(f"üìÑ Gefundene CSV-Dateien: {len(csv_files)}")
    print(f"üìÑ Gefundene PDF-Dateien: {len(pdf_files)}")

    if not pdf_files:
        raise FileNotFoundError("‚ùå Keine PDF-Datei gefunden. Bitte stelle sicher, dass mindestens eine PDF im Ordner liegt.")

    return csv_files, pdf_files

def detect_separator(filepath: str) -> str:
    """
    Erkennt automatisch das Trennzeichen in der ersten Zeile der Datei.
    Unterst√ºtzt ',', ';' und Tabulator ('\\t').
    """
    with open(filepath, encoding="utf-8") as f:
        first_line = f.readline()

    # Z√§hle Vorkommen m√∂glicher Trenner
    candidates = {",": first_line.count(","), ";": first_line.count(";"), "\t": first_line.count("\t")}

    # W√§hle den Trenner mit den meisten Vorkommen
    best = max(candidates, key=candidates.get)

    # Warnung, falls alles gleich 0 ist (z.‚ÄØB. eine leere oder ungew√∂hnliche Datei)
    if candidates[best] == 0:
        print("‚ö†Ô∏è Kein g√§ngiges Trennzeichen erkannt, Standardwert ',' wird verwendet.")
        return ","

    return best

def parse_date_string(date_str: str) -> pd.Timestamp | None:
    """Konvertiert Strings wie '2010', 'heute', 'today' in ein Datum, sonst None."""
    date_str = date_str.strip().lower()
    today = pd.Timestamp.today().normalize()

    if date_str in {"heute", "today"}:
        return today

    try:
        return pd.to_datetime(date_str, errors="raise", dayfirst=True)
    except Exception:
        return None
    
def parse_period_string(entry: str) -> tuple[pd.Timestamp, pd.Timestamp] | None:
    """
    Erkennt entweder:
    - Einzelzeitpunkt wie '2010', 'heute' ‚Üí Start = End
    - Zeitraum wie '2010 - 2015' ‚Üí Start, End
    Gibt: (start, end) oder None
    """
    entry = entry.strip()

    # Trenne Bereich
    split_match = re.split(r"\s*[-‚Äìbis]+\s*", entry, maxsplit=1)
    if len(split_match) == 1:
        date = parse_date_string(split_match[0])
        return (date, date) if date else None
    elif len(split_match) == 2:
        start = parse_date_string(split_match[0])
        end = parse_date_string(split_match[1])
        return (start, end) if start and end else None
    return None

def quality_check(df: pd.DataFrame, time_column: str = 'Timeline') -> bool:
    """
    Pr√ºft ob:
    - Zeitspalte vorhanden
    - mind. 4 g√ºltige Eintr√§ge (einzelne Zeitpunkte oder Zeitr√§ume)
    - Gesamtzeitraum >= 1 Jahr
    """
    if time_column not in df.columns:
        print(f"‚ùå Zeitspalte '{time_column}' nicht gefunden.")
        return False

    if len(df) < 4:
        print(f"‚ùå Der DataFrame enth√§lt nur {len(df)} Zeilen (mindestens 4 erforderlich).")
        return False

    df = df.copy()
    periods = df[time_column].astype(str).apply(parse_period_string)
    valid_periods = [p for p in periods if p is not None]

    if len(valid_periods) < 4:
        print(f"‚ùå Nur {len(valid_periods)} g√ºltige Zeitangaben erkannt (mindestens 4 erforderlich).")
        return False

    starts, ends = zip(*valid_periods)
    overall_start = min(starts)
    overall_end = max(ends)
    duration = overall_end - overall_start

    if duration < pd.Timedelta(days=365):
        print(f"‚ùå Zeitraum ist zu kurz: nur {duration.days} Tage (mind. 1 Jahr erforderlich).")
        return False

    print(f"‚úÖ Es wurden {len(df)} Berufserfahrungen erkannt. Der Zeitraum erstreckt sich von {overall_start.strftime('%B %Y')} bis {overall_end.strftime('%B %Y')} ({(overall_end - overall_start).days} Tage).")
    print("‚úÖ Qualit√§tspr√ºfung bestanden.")
    return True

def load_resume_data(filepath: str) -> pd.DataFrame:
    
    sep = detect_separator(filepath)

    try:
        # Korrektur: Header ist in Zeile 0
        df = pd.read_csv(filepath, sep=sep, header=0)
    except Exception as e:
        raise ValueError(f"‚ùå Fehler beim Einlesen der CSV: {e}")

    # Leere Spalten entfernen
    df = df.drop(columns=[col for col in df.columns if col.startswith("Unnamed")], errors="ignore")

    # Leere Zeilen entfernen
    df = df.dropna(how="all")

    # Whitespace aus Spaltennamen entfernen
    df.columns = [col.strip() for col in df.columns]

    # ToDo: Hier noch einen Qualit√§tscheck einf√ºgen. z.B. Datensatz nach Relevanz filtern --> vorgeschriebene Spalten oder mit Chatgpt Spaltennamen nach Relevanz auswerten
    quality_check(df) 
    return df

### Einlesen PDF

def normalize_pdf_text(text: str) -> str:
    """
    Bereinigt gestreckten, duplizierten oder zerschossenen Text aus PDF-Scans.
    - Fasst Buchstabenwiederholungen wie 'TTTeeecccchhhh' zu 'Tech' zusammen
    - Entfernt mehrfach wiederholte W√∂rter oder Bl√∂cke
    - Vereinheitlicht Leerzeichen
    """

    # 1. Einzelne Buchstaben-Wiederholungen reduzieren (z.B. SSSoooffftttwwwaaarrreee ‚Üí Software)
    text = re.sub(r'(.)\1{2,}', r'\1', text)

    # 2. √úberlange Wortwiederholungen zusammenfassen
    text = re.sub(r'\b(\w{3,})\s+\1\b', r'\1', text)

    # 3. Whitespace bereinigen
    text = re.sub(r'\s+', ' ', text)

    # 4. Erste Buchstaben wieder gro√ü schreiben (optional)
    text = re.sub(r'(^|\.\s+)([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)

    # 5. Trim
    text = text.strip()

    return text

def extract_clean_text_from_pdf(pdf_path: str) -> str:
    """
    Extrahiert und bereinigt den Text aus einer PDF-Datei mit pdfplumber.
    Falls dabei kein Text herauskommt, wird OCR nachgeladen und erneut extrahiert.
    
    Vorteile gegen√ºber PyPDF2:
    - Bessere Textstruktur
    - Weniger Layout-M√ºll
    - Keine zerrissenen W√∂rter oder Duplikate
    
    R√ºckgabe:
        str: Bereinigter Text der PDF
    """
    def _read_pdf(path: str) -> str:
        with pdfplumber.open(path) as pdf:
            return " ".join(page.extract_text() or "" for page in pdf.pages)

    # 1) Erster Versuch ohne OCR
    try:
        raw_text = _read_pdf(pdf_path)
    except Exception as e:
        raise ValueError(f"‚ùå Fehler beim √ñffnen oder Lesen der PDF-Datei: {e}")

    cleaned_text = normalize_pdf_text(raw_text)

    # 2) Falls kein sinnvoller Text, OCR-Fallback
    ocr_pdf_path = False
    pattern = re.compile(r'^(?:\b\w+\b\W+){49}\b\w+\b', flags=re.UNICODE)
    if not bool(pattern.match(cleaned_text.strip())):
    #if not cleaned_text.strip():
        print("‚ÑπÔ∏è Kein Text gefunden, f√ºhre OCR nach ...")
        fd, ocr_pdf_path = tempfile.mkstemp(suffix=".pdf")
        os.close(fd)
        try:
            # force_ocr sorgt daf√ºr, dass auch textbasierte PDFs verarbeitet werden
           ocrmypdf.ocr(
                pdf_path,
                ocr_pdf_path,
                output_type="pdf",
                force_ocr=True,
                skip_text=False,
                redo_ocr=False
            )
           raw_text = _read_pdf(ocr_pdf_path)
           cleaned_text = normalize_pdf_text(raw_text)
           if cleaned_text.strip():
                print("‚úÖ OCR erfolgreich, Text extrahiert.")
           else:
               print("‚ö†Ô∏è Auch nach OCR konnte kein Text extrahiert werden.")

        except Exception as oe:
            raise ValueError(f"‚ùå OCR-Fehler: {oe}")

    if not cleaned_text.strip():
        raise ValueError("‚ùå Es konnte kein sinnvoller Text aus der PDF extrahiert werden.")
    
    print("‚úÖ Die Stellenausschreibung wurde erkannt.")
    return cleaned_text, ocr_pdf_path

def _get_cache_path(key: str) -> str:
        return os.path.join(CACHE_DIR, f"{key}.txt")

def load_cached_reduction(key: str) -> str | None:
        os.makedirs(CACHE_DIR, exist_ok=True)
        path = _get_cache_path(key)
        if os.path.exists(path):
            print("üîÑ Verwende gecachte Reduktion.")
            return open(path, "r", encoding="utf-8").read()
        return None

def save_cached_reduction(key: str, text: str):
        os.makedirs(CACHE_DIR, exist_ok=True)
        with open(_get_cache_path(key), "w", encoding="utf-8") as f:
            f.write(text)

def make_key_from_file(pdf_path: str) -> str:
    buf = open(pdf_path, "rb").read()
    return hashlib.sha256(buf).hexdigest()

def reduce_pdf_to_essentials(text: str, pdf_path : str, cache_key: str) -> str:
    """
    Reduziert den PDF-Text auf das Wesentliche via ChatGPT, mit einfachem Cache.
    """
    prompt = (
        "Ich gebe dir eine Stellenanzeige:\n\n"
        f"{text}\n\n"
        "Bitte fasse diesen Text pr√§zise zusammen und hebe **besonders** die technischen Anforderungen hervor, "
        "insbesondere:\n"
        "- Programmiersprachen und Versionen\n"
        "- Frameworks und Bibliotheken\n"
        "- Tools und Plattformen\n"
        "- Schnittstellen oder APIs und Protokolle\n"
        "- T√§tigkeitsbereich des Unternehmens\n\n"
        "Die Zusammenfassung soll in Stichpunkten erfolgen."
    )

    prompt_length_bool = validate_prompt_length(text)
    
    if not prompt_length_bool:
        chunks = chunk_text_by_tokens(text)
        for chunk in chunks:
            reduced = ""
            prompt = (
            f"Ich gebe dir einen Auszug aus einer Stellenanzeige:\n\n"
            f"{chunk}\n\n"
            "Bitte fasse diesen Text pr√§zise zusammen und hebe **besonders** die technischen Anforderungen hervor, "
            "insbesondere:\n"
            "- Programmiersprachen und Versionen\n"
            "- Frameworks und Bibliotheken\n"
            "- Tools und Plattformen\n"
            "- Schnittstellen oder APIs und Protokolle\n"
            "- T√§tigkeitsbereich des Unternehmens\n\n"
            "Die Zusammenfassung soll in Stichpunkten erfolgen."
        )
            chunk_reduced = ask_chatgpt_single_prompt(prompt)
            reduced = reduced + " " + chunk_reduced

            save_cached_reduction(cache_key, reduced)

            print("‚úÖ PDF-Inhalt wurde erfolgreich reduziert.")
            return reduced

    reduced = ask_chatgpt_single_prompt(prompt)

    save_cached_reduction(cache_key, reduced)

    print("‚úÖ PDF-Inhalt wurde erfolgreich reduziert.")
    return reduced

def get_pdf_page_count(pdf_path: str) -> int:
    """Z√§hlt die Seiten einer PDF."""
    with pdfplumber.open(pdf_path) as pdf:
        return len(pdf.pages)

def extract_headings_from_pdf(pdf_path: str) -> list[str]:
    """
    Liest jede Seite mit pdfplumber ein, splittet sie in Zeilen
    und sammelt alle Zeilen, die mit ‚ÄûZahl. Text‚Äú beginnen.
    """
    headings: list[str] = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            for line in text.split("\n"):
                clean = line.strip()
                # Regex: Zeilenanfang, Zahl+, optional weitere .Ziffern, Punkt, mindestens ein Leerzeichen, dann Text
                if re.match(r'^\d+(?:\.\d+)*\.\s+\S+', clean):
                    headings.append(clean)
    return headings

def select_best_heading(headings):
    """
    Fragt ChatGPT, welche Heading(s) am relevantesten f√ºr die Stellenanzeige sind.
    Liefert eine ganz normale Python-Liste von Kapitel-Titeln zur√ºck.
    """
    # 1) Inhaltsverzeichnis als Bullet-Point-Liste f√ºr den Prompt aufbauen
    toc = "\n".join(f"- {h}" for h in headings)
    prompt = (
        "Die PDF enth√§lt folgende Kapitel√ºberschriften:\n"
        f"{toc}\n\n"
        "Welche dieser √úberschriften sind essentiell f√ºr die Anforderungen und Qualifikation des "
        "Personals im Projekt? Bitte antworte **nur** mit den exakten Kapitel-Titeln, jeder Titel "
        "in einer neuen Zeile. Du darfst daf√ºr '-' voranstellen oder weglassen."
    )

    # 2) ChatGPT fragen
    raw = ask_chatgpt_single_prompt(prompt).strip()

    # 3) Antwort in Zeilen splitten und leerzeilen herausfiltern
    lines = [line.strip() for line in raw.splitlines() if line.strip()]

    # 4) f√ºhrendes "- " entfernen (falls vorhanden) und saubere Liste sammeln
    result = []
    for line in lines:
        if line.startswith("-"):
            result.append(line.lstrip("- ").strip())
        else:
            result.append(line)
    return result

def extract_chapter_text(full_text, chosen, all_headings):
    """
    Schneidet aus full_text nur den Abschnitt heraus, der zu `chosen` geh√∂rt.
    Wenn `chosen` eine Liste von √úberschriften ist, werden alle entsprechenden
    Abschnitte extrahiert und hintereinander geh√§ngt.
    """
    def _extract_one(text, heading, headings):
        # Split vor der gew√§hlten √úberschrift
        parts = re.split(rf'(?m)^{re.escape(heading)}.*$', text, maxsplit=1)
        if len(parts) < 2:
            return ""
        after = parts[1]
        # Finde Position der n√§chsten √úberschrift
        others = [h for h in headings if h != heading]
        if others:
            nxt_pattern = rf'(?m)^({"|".join(re.escape(h) for h in others)})'
            m = re.search(nxt_pattern, after)
            if m:
                return after[:m.start()].strip()
        return after.strip()

    # Handle list of headings
    if isinstance(chosen, (list, tuple)):
        sections = []
        for heading in chosen:
            sec = _extract_one(full_text, heading, all_headings)
            if sec:
                sections.append(sec)
        return "\n\n".join(sections)

    # Single heading
    return _extract_one(full_text, chosen, all_headings)

def process_pdf(pdf_path: str) -> str:
    """
    - Nutzt Cache, sofern vorhanden.
    - Liest PDF, falls ‚â§3 Seiten: Volltext ‚Üí reduce.
    - Falls >3 Seiten: sucht Kapitel, fragt ChatGPT, extrahiert bestes Kapitel ‚Üí reduce.
    """
    # 1) Cache-Key pr√ºfen
    key = make_key_from_file(pdf_path)
    cached = load_cached_reduction(key)
    if cached is not None:
        return cached

    # 2) PDF √∂ffnen und Volltext sammeln
    full_text, ocr_pdf_path = extract_clean_text_from_pdf(pdf_path)
    token_count = count_tokens(full_text)
    
    # 3) Bei kurzen PDFs: direkt reduzieren
    if token_count <= 2000:
        reduced = reduce_pdf_to_essentials(full_text, pdf_path, cache_key=key)
        save_cached_reduction(key, reduced)
        return reduced

    # 4) Bei langen PDFs: Kapitel-Handling
    if ocr_pdf_path:
        headings = extract_headings_from_pdf(ocr_pdf_path)
    else:
        headings = extract_headings_from_pdf(pdf_path)

    if not headings:
        # Falls keine Kapitel gefunden: wie Kurz-PDF behandeln
        reduced = reduce_pdf_to_essentials(full_text, pdf_path, cache_key=key)
        
    if headings:
        best = select_best_heading(headings)
        #print(f"‚ÑπÔ∏è W√§hle Kapitel ‚Äû{best}‚Äú f√ºr Reduktion.")
        full_text = extract_chapter_text(full_text, best, headings)
        reduced = reduce_pdf_to_essentials(full_text, pdf_path, cache_key=key)
    
    # 5) Dieses Kapitel reduzieren und cachen
    save_cached_reduction(key, reduced)
    return reduced

